{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SppYpQ8DJWjf"
      },
      "source": [
        "**Download dataset from Roboflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aL4Iuh_Zpvfu",
        "outputId": "25758dee-2a6d-4780-cdd4-89d7aba2ff41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.12-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2023.7.22 (from roboflow)\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Collecting pyparsing==2.4.7 (from roboflow)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Collecting supervision (from roboflow)\n",
            "  Downloading supervision-0.17.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-magic (from roboflow)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.45.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: scipy<=2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.4)\n",
            "Installing collected packages: python-magic, python-dotenv, pyparsing, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.8.1.78\n",
            "    Uninstalling opencv-python-headless-4.8.1.78:\n",
            "      Successfully uninstalled opencv-python-headless-4.8.1.78\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 pyparsing-2.4.7 python-dotenv-1.0.0 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.12 supervision-0.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Food-7 to yolov5pytorch:: 100%|██████████| 1914/1914 [00:00<00:00, 3256.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Food-7 in yolov5pytorch:: 100%|██████████| 90/90 [00:00<00:00, 8106.68it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"lttzJNap0h9lODifvr4O\")\n",
        "project = rf.workspace(\"school-yrws4\").project(\"food-pion4\")\n",
        "dataset = project.version(7).download(\"yolov5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41DglgncJiNq"
      },
      "source": [
        "**Write on CSV file with the annotation files**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSK8ef2gKG7v",
        "outputId": "314b7110-15cb-42ee-a95d-d7b43ea9ed5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXL5oLVqEQ7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def process_label_file(file_path):\n",
        "    objects = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 5:  # Ensure there are 5 elements in the line\n",
        "                object_id, x_center, y_center, width, height = map(float, parts)\n",
        "                objects.append([object_id, x_center, y_center, width, height, '', '', ''])  # Leaving area and mass empty\n",
        "    return objects\n",
        "\n",
        "def get_processed_images(csv_file):\n",
        "    processed_images = set()\n",
        "    if os.path.exists(csv_file):\n",
        "        with open(csv_file, 'r') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            next(reader, None)  # Skip header\n",
        "            for row in reader:\n",
        "                if row:\n",
        "                    processed_images.add(row[0])\n",
        "    return processed_images\n",
        "\n",
        "def adjust_image_name(image_name):\n",
        "    for ext in ['jpg', 'jpeg']:\n",
        "        index = image_name.find('_' + ext)\n",
        "        if index != -1:\n",
        "            return image_name[:index]\n",
        "    return image_name  # Return original name if no match\n",
        "\n",
        "def process_dataset(dataset_path, output_csv):\n",
        "    processed_images = get_processed_images(output_csv)\n",
        "    data_to_write = []\n",
        "    for folder in ['train', 'test', 'valid']:\n",
        "        label_path = os.path.join(dataset_path, folder, 'labels')\n",
        "        for label_file in os.listdir(label_path):\n",
        "            if label_file.endswith('.txt'):\n",
        "                image_name = label_file.replace('.txt', '')\n",
        "                adjusted_image_name = adjust_image_name(image_name)\n",
        "                if adjusted_image_name not in processed_images:\n",
        "                    objects = process_label_file(os.path.join(label_path, label_file))\n",
        "                    for obj in objects:\n",
        "                        data_to_write.append([adjusted_image_name] + obj)\n",
        "\n",
        "    return data_to_write\n",
        "\n",
        "def write_to_csv(data, output_csv):\n",
        "    if_exist = os.path.exists(output_csv)\n",
        "    mode = 'a' if if_exist else 'w'\n",
        "    with open(output_csv, mode, newline='') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        if if_exist == False:\n",
        "          csv_writer.writerow(['image_name', 'object_id', 'x_center', 'y_center', 'width', 'height', 'image_area', 'area', 'mass'])\n",
        "        csv_writer.writerows(data)\n",
        "\n",
        "def sort_csv_data(csv_file):\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        sorted_data = sorted(reader, key=lambda x: x[0])  # Sort by image name\n",
        "\n",
        "    with open(csv_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(sorted_data)\n",
        "\n",
        "\n",
        "dataset_path = os.path.join(HOME, \"Food-7\")\n",
        "output_csv = os.path.join(HOME, \"output.csv\")\n",
        "data_to_write = process_dataset(dataset_path, output_csv)\n",
        "\n",
        "write_to_csv(data_to_write, output_csv)\n",
        "sort_csv_data(output_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhNby4YDJrxJ"
      },
      "source": [
        "**Load SAM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVWjABr8un18",
        "outputId": "33031d87-5fec-4c9e-e234-edd6a8617861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcc80WnZuzDr",
        "outputId": "fe09108b-aa39-409a-be80-d6c01bcd35cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZMnoiHkvAK8"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/weights\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEKR4M18vGGh",
        "outputId": "0a7fada2-a619-478e-a0dd-cba816b7d6d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/weights/sam_vit_h_4b8939.pth ; exist: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLIUAhpKvOLE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDhPbT89vUit"
      },
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RbJ6D_Zv6Bm"
      },
      "outputs": [],
      "source": [
        "mask_predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bt6isPfJz3B"
      },
      "source": [
        "**Calculate areas and rewrite on CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEHmImcFwncl"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import csv\n",
        "import os\n",
        "import supervision as v\n",
        "\n",
        "#     # Replace the original file with the updated temp file\n",
        "#     os.remove(csv_file)\n",
        "#     os.rename(temp_file, csv_file)\n",
        "\n",
        "def process_images_and_create_masks(dataset_path, csv_file):\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)  # Skip header\n",
        "        csv_data = list(reader)\n",
        "\n",
        "    rows_temp = []\n",
        "    for folder in ['train', 'test', 'valid']:\n",
        "        image_folder_path = os.path.join(dataset_path, folder, 'images')\n",
        "        for image_file in os.listdir(image_folder_path):\n",
        "            image_name = os.path.splitext(image_file)[0]\n",
        "            adjusted_image_name = adjust_image_name(image_name)\n",
        "\n",
        "            IMAGE_PATH = os.path.join(image_folder_path, image_file)\n",
        "            image_bgr = cv2.imread(IMAGE_PATH)\n",
        "            image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            mask_predictor.set_image(image_rgb)\n",
        "            info = []\n",
        "            for row in csv_data:\n",
        "                if row[0] == adjusted_image_name:\n",
        "                    x_center, y_center, width, height = map(float, row[2:6])\n",
        "                    id = row[1]\n",
        "                    x_min = (x_center * 640) - (width *640) / 2\n",
        "                    y_min = (y_center * 640) - (height *640) / 2\n",
        "                    x_max = (x_center * 640) + (width *640) / 2\n",
        "                    y_max = (y_center * 640) + (height *640) / 2\n",
        "\n",
        "                    bbox = np.array([x_min, y_min, x_max, y_max])\n",
        "\n",
        "                    masks, scores, logits = mask_predictor.predict(\n",
        "                        box=bbox,\n",
        "                        multimask_output= True\n",
        "                    )\n",
        "                    image_area = np.count_nonzero(masks[2])\n",
        "\n",
        "                    # visualize the mask with sv\n",
        "                    # sv.plot_images_grid(\n",
        "                    #     images=masks,\n",
        "                    #     grid_size=(1, 4),\n",
        "                    #     size=(16, 4)\n",
        "                    # )\n",
        "\n",
        "                    # box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n",
        "                    # mask_annotator = sv.MaskAnnotator(color=sv.Color.red(), color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "                    # detections = sv.Detections(\n",
        "                    #     xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "                    #     mask=masks\n",
        "                    # )\n",
        "                    # detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "                    # source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections, skip_label=True)\n",
        "                    # segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "                    # sv.plot_images_grid(\n",
        "                    #     images=[source_image, segmented_image],\n",
        "                    #     grid_size=(1, 2),\n",
        "                    #     titles=['source image', 'segmented image']\n",
        "                    # )\n",
        "\n",
        "                    # print(f\"{adjusted_image_name} {id} {image_area}\")\n",
        "                    info.append(row[:6] + [image_area])\n",
        "\n",
        "\n",
        "            coin_image = 0\n",
        "            coin_area = (13**2) * np.pi  # coin area in mm^2\n",
        "            for obj in info:\n",
        "                if float(obj[1])==2.0:\n",
        "                    coin_image = obj[6]\n",
        "\n",
        "            for obj in info:\n",
        "                if coin_image!=0:\n",
        "                    S = obj[6] / coin_image * coin_area\n",
        "                    rows_temp.append(obj + [S, ''])\n",
        "                else:\n",
        "                    rows_temp.append(obj + ['', ''])\n",
        "\n",
        "            # Reset image\n",
        "            mask_predictor.reset_image()\n",
        "            del image_bgr\n",
        "            del image_rgb\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    with open(csv_file, 'w') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['image_name', 'object_id', 'x_center', 'y_center', 'width', 'height', 'image_area', 'area', 'mass'])\n",
        "        writer.writerows(rows_temp)\n",
        "    sort_csv_data(csv_file)\n",
        "\n",
        "dataset_path = os.path.join(HOME, \"Food-7\")\n",
        "csv_file = os.path.join(HOME, \"output.csv\")\n",
        "\n",
        "process_images_and_create_masks(dataset_path, csv_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}